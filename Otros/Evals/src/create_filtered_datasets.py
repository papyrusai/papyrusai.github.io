import pandas as pd
import os

def create_filtered_datasets():
    """
    Crea 3 datasets filtrados basÃ¡ndose en el campo 'Ãrea' del dataset de Notion
    y cruzÃ¡ndolo con el dataset de Langsmith enhanced v2.
    """
    
    print("CREANDO DATASETS FILTRADOS")
    print("=" * 50)
    
    # Cargar datasets
    try:
        df_notion = pd.read_csv('data/Dataset_notion_1620_01_08.csv')
        df_langsmith = pd.read_csv('data/Dataset_langsmith_enhanced_v2.csv')
        
        print(f"âœ… Dataset Notion cargado: {len(df_notion)} filas")
        print(f"âœ… Dataset Langsmith enhanced v2 cargado: {len(df_langsmith)} filas")
        
    except Exception as e:
        print(f"âŒ Error cargando datasets: {e}")
        return
    
    # FunciÃ³n para extraer DocID del campo input_question de Langsmith
    def extract_doc_id_from_langsmith(input_text):
        if pd.isna(input_text) or not isinstance(input_text, str):
            return None
        # Buscar patrÃ³n DocXX: al inicio del texto
        import re
        match = re.match(r'^(Doc\d+)', input_text.strip())
        if match:
            return match.group(1)
        return None
    
    # Extraer DocIDs del dataset de Langsmith
    df_langsmith['DocID'] = df_langsmith['input_question'].apply(extract_doc_id_from_langsmith)
    
    print(f"\nðŸ“Š DocIDs extraÃ­dos de Langsmith: {df_langsmith['DocID'].notna().sum()}")
    
    # 1. DATASET FALTA: Ãrea empieza por "Falta"
    print("\nðŸ” PROCESANDO DATASET FALTA:")
    print("-" * 30)
    
    # Filtrar filas de Notion donde Ãrea empieza por "Falta"
    notion_falta = df_notion[df_notion['Ãrea'].str.startswith('Falta', na=False)]
    falta_doc_ids = notion_falta['ID'].tolist()
    
    print(f"DocIDs en Notion con Ã¡rea 'Falta': {len(falta_doc_ids)}")
    print(f"Lista: {falta_doc_ids}")
    
    # Cruzar con Langsmith
    langsmith_falta = df_langsmith[df_langsmith['DocID'].isin(falta_doc_ids)]
    
    # Guardar dataset
    langsmith_falta.to_csv('data/Dataset_langsmith_falta.csv', index=False)
    print(f"âœ… Dataset_langsmith_falta.csv creado: {len(langsmith_falta)} filas")
    
    # 2. DATASET GRISES: Ãrea contiene "Gris"
    print("\nðŸ” PROCESANDO DATASET GRISES:")
    print("-" * 30)
    
    # Filtrar filas de Notion donde Ãrea contiene "Gris"
    notion_grises = df_notion[df_notion['Ãrea'].str.contains('Gris', na=False, case=False)]
    grises_doc_ids = notion_grises['ID'].tolist()
    
    print(f"DocIDs en Notion con Ã¡rea que contiene 'Gris': {len(grises_doc_ids)}")
    print(f"Lista: {grises_doc_ids}")
    
    # Cruzar con Langsmith
    langsmith_grises = df_langsmith[df_langsmith['DocID'].isin(grises_doc_ids)]
    
    # Guardar dataset
    langsmith_grises.to_csv('data/Dataset_langsmith_grises.csv', index=False)
    print(f"âœ… Dataset_langsmith_grises.csv creado: {len(langsmith_grises)} filas")
    
    # 3. DATASET RUIDO: Ãrea empieza por "Ruido" O contiene "Correccion"
    print("\nðŸ” PROCESANDO DATASET RUIDO:")
    print("-" * 30)
    
    # Filtrar filas de Notion donde Ãrea empieza por "Ruido" O contiene "Correccion"
    condition_ruido = (
        df_notion['Ãrea'].str.startswith('Ruido', na=False) |
        df_notion['Ãrea'].str.contains('Correccion', na=False, case=False)
    )
    notion_ruido = df_notion[condition_ruido]
    ruido_doc_ids = notion_ruido['ID'].tolist()
    
    print(f"DocIDs en Notion con Ã¡rea 'Ruido' o 'CorrecciÃ³n': {len(ruido_doc_ids)}")
    print(f"Lista: {ruido_doc_ids}")
    
    # Cruzar con Langsmith
    langsmith_ruido = df_langsmith[df_langsmith['DocID'].isin(ruido_doc_ids)]
    
    # Guardar dataset
    langsmith_ruido.to_csv('data/Dataset_langsmith_ruido.csv', index=False)
    print(f"âœ… Dataset_langsmith_ruido.csv creado: {len(langsmith_ruido)} filas")
    
    # RESUMEN FINAL
    print("\nðŸ“‹ RESUMEN DE DATASETS CREADOS:")
    print("=" * 40)
    print(f"ðŸ“ Dataset_langsmith_falta.csv: {len(langsmith_falta)} filas")
    print(f"ðŸ“ Dataset_langsmith_grises.csv: {len(langsmith_grises)} filas")
    print(f"ðŸ“ Dataset_langsmith_ruido.csv: {len(langsmith_ruido)} filas")
    
    total_filtered = len(langsmith_falta) + len(langsmith_grises) + len(langsmith_ruido)
    print(f"\nðŸ“Š Total filas en datasets filtrados: {total_filtered}")
    print(f"ðŸ“Š Dataset original Langsmith: {len(df_langsmith)} filas")
    
    # Mostrar algunos ejemplos de cada categorÃ­a
    print("\nðŸ” EJEMPLOS DE CLASIFICACIÃ“N:")
    print("-" * 30)
    
    if len(notion_falta) > 0:
        print(f"âœ… FALTA - Ejemplo: {notion_falta.iloc[0]['ID']} -> '{notion_falta.iloc[0]['Ãrea']}'")
    
    if len(notion_grises) > 0:
        print(f"ðŸ”˜ GRISES - Ejemplo: {notion_grises.iloc[0]['ID']} -> '{notion_grises.iloc[0]['Ãrea']}'")
        
    if len(notion_ruido) > 0:
        print(f"ðŸ”‡ RUIDO - Ejemplo: {notion_ruido.iloc[0]['ID']} -> '{notion_ruido.iloc[0]['Ãrea']}'")
    
    # Verificar que no hay duplicados entre categorÃ­as
    print("\nðŸ” VERIFICACIÃ“N DE SOLAPAMIENTOS:")
    print("-" * 35)
    
    set_falta = set(falta_doc_ids)
    set_grises = set(grises_doc_ids)
    set_ruido = set(ruido_doc_ids)
    
    overlap_falta_grises = set_falta.intersection(set_grises)
    overlap_falta_ruido = set_falta.intersection(set_ruido)
    overlap_grises_ruido = set_grises.intersection(set_ruido)
    
    print(f"Solapamiento Falta-Grises: {len(overlap_falta_grises)} DocIDs")
    if overlap_falta_grises:
        print(f"  -> {list(overlap_falta_grises)}")
        
    print(f"Solapamiento Falta-Ruido: {len(overlap_falta_ruido)} DocIDs")
    if overlap_falta_ruido:
        print(f"  -> {list(overlap_falta_ruido)}")
        
    print(f"Solapamiento Grises-Ruido: {len(overlap_grises_ruido)} DocIDs")
    if overlap_grises_ruido:
        print(f"  -> {list(overlap_grises_ruido)}")
    
    print(f"\nâœ… Proceso completado exitosamente")
    print(f"ðŸ“‚ Los datasets originales se mantienen intactos")
    print(f"ðŸ“‚ Nuevos datasets creados en la carpeta data/")

if __name__ == "__main__":
    # Cambiar al directorio Evals
    script_dir = os.path.dirname(os.path.abspath(__file__))
    evals_dir = os.path.dirname(script_dir)
    os.chdir(evals_dir)
    print(f"Directorio de trabajo: {os.getcwd()}")
    create_filtered_datasets() 